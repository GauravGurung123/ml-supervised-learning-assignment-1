{
 "cells": [
  {
   "cell_type": "code",
   "id": "5ceeb33d-100a-4d42-980b-c87f97332e35",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a395066-98df-40eb-95de-a8925d073d27",
   "metadata": {},
   "source": [
    "# Grid world parameters\n",
    "GRID_ROWS = 10\n",
    "GRID_COLS = 10\n",
    "\n",
    "N_STATES = GRID_ROWS * GRID_COLS  # 100 个状态\n",
    "\n",
    "START_STATE = 1     # 起始状态（左上角）\n",
    "GOAL_STATE = 100    # 目标状态（右下角）\n",
    "\n",
    "# Actions: 0 = up, 1 = down, 2 = left, 3 = right\n",
    "ACTIONS = [0, 1, 2, 3]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0860899f-86b0-4125-8008-e19be6750fbb",
   "metadata": {},
   "source": [
    "def state_to_position(s):\n",
    "  \n",
    "    s_index = s - 1  # 变成 0~99\n",
    "    row = s_index // GRID_ROWS\n",
    "    col = s_index % GRID_COLS\n",
    "    return row, col\n",
    "\n",
    "\n",
    "def position_to_state(row, col):\n",
    "   \n",
    "    return row * GRID_COLS + col + 1\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "817b21ba-9eba-4460-a8b7-d95b5acf3004",
   "metadata": {},
   "source": [
    "def step(state, action):\n",
    "   \n",
    "    row, col = state_to_position(state)\n",
    "\n",
    "    if action == 0:      # up\n",
    "        new_row, new_col = row - 1, col\n",
    "    elif action == 1:    # down\n",
    "        new_row, new_col = row + 1, col\n",
    "    elif action == 2:    # left\n",
    "        new_row, new_col = row, col - 1\n",
    "    elif action == 3:    # right\n",
    "        new_row, new_col = row, col + 1\n",
    "    else:\n",
    "        raise ValueError(\"Unknown action!\")\n",
    "\n",
    "    # 边界条件：不能走出 0~9 的范围，超出就停在原地\n",
    "    if new_row < 0 or new_row >= GRID_ROWS or new_col < 0 or new_col >= GRID_COLS:\n",
    "        new_row, new_col = row, col  # 保持不变\n",
    "\n",
    "    next_state = position_to_state(new_row, new_col)\n",
    "    return next_state\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2bc34e6-982a-4dcb-b4bd-7a45c872d544",
   "metadata": {},
   "source": [
    "def reward(state):\n",
    "   \n",
    "    if state == GOAL_STATE:\n",
    "        return 100.0\n",
    "    else:\n",
    "        return 0.0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bd98887-aff1-4f8b-9b23-7065cb7a93f1",
   "metadata": {},
   "source": [
    "def run_episode_random(max_steps=1000):\n",
    "  \n",
    "    current_state = START_STATE\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # 随机选一个动作\n",
    "        action = np.random.choice(ACTIONS)\n",
    "\n",
    "        # 根据状态转移函数得到下一个状态\n",
    "        next_state = step(current_state, action)\n",
    "\n",
    "        # 根据新状态计算奖励\n",
    "        r = reward(next_state)\n",
    "\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "\n",
    "        # 如果到达目标状态，本回合结束\n",
    "        if next_state == GOAL_STATE:\n",
    "            break\n",
    "\n",
    "        # 否则继续\n",
    "        current_state = next_state\n",
    "\n",
    "    # 平均每步奖励（避免除 0）\n",
    "    avg_reward_per_step = total_reward / steps if steps > 0 else 0.0\n",
    "\n",
    "    return avg_reward_per_step, steps\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a95f0274-d044-4ca6-8198-b21708558600",
   "metadata": {},
   "source": [
    "def run_multiple_episodes(n_runs=30, max_steps=1000):\n",
    "   \n",
    "    avg_rewards = []\n",
    "    steps_list = []\n",
    "    runtimes = []\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        avg_r, steps = run_episode_random(max_steps=max_steps)\n",
    "\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "\n",
    "        avg_rewards.append(avg_r)\n",
    "        steps_list.append(steps)\n",
    "        runtimes.append(runtime)\n",
    "\n",
    "        print(f\"Run {i+1}/{n_runs}: avg_reward = {avg_r:.4f}, steps = {steps}, runtime = {runtime:.4f} s\")\n",
    "\n",
    "    return np.array(avg_rewards), np.array(steps_list), np.array(runtimes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87f5ea1c-a845-4b37-bbdf-70a2d5b40821",
   "metadata": {},
   "source": [
    "avg_rewards, steps_list, runtimes = run_multiple_episodes(n_runs=30, max_steps=1000)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4b251af-eb9f-4e4c-b249-1957f7236916",
   "metadata": {},
   "source": [
    "def summarize_results(avg_rewards, steps_list, runtimes):\n",
    "  \n",
    "    print(\"=== Summary over runs ===\")\n",
    "    print(f\"Average reward per step: mean = {avg_rewards.mean():.4f}, std = {avg_rewards.std():.4f}\")\n",
    "    print(f\"Steps to goal:          mean = {steps_list.mean():.2f}, std = {steps_list.std():.2f}\")\n",
    "    print(f\"Runtime (seconds):      mean = {runtimes.mean():.4f}, std = {runtimes.std():.4f}\")\n",
    "\n",
    "\n",
    "summarize_results(avg_rewards, steps_list, runtimes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "153df7a9-3b4a-4a59-a494-a3ce5305e64f",
   "metadata": {},
   "source": [
    "# 为了图更好看一点，可以把图像放大一些\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# 1. 平均奖励的 boxplot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot(avg_rewards)\n",
    "plt.title(\"Avg Reward per Step\")\n",
    "\n",
    "# 2. 步数的 boxplot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(steps_list)\n",
    "plt.title(\"Steps to Goal\")\n",
    "\n",
    "# 3. 运行时间的 boxplot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(runtimes)\n",
    "plt.title(\"Runtime (s)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81a9c6db-f8e5-43c5-a4ff-5e1feef2c899",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2 – Q-learning\n",
    "\n",
    "在这一部分中，我们在与 Exercise 1 相同的 10×10 网格环境中，引入 **Q-learning 算法**，\n",
    "让智能体通过试错学习每个状态–动作对的价值，并在不同训练步数时测试其表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45e892-7d30-48c7-91cc-99f6805ab399",
   "metadata": {},
   "source": [
    "## 2.1 Initialize Q-table\n",
    "\n",
    "在这一小节中，我们创建一个大小为 (100 × 4) 的 Q 表，用来存储每个状态–动作对的估计价值，\n",
    "并将所有元素初始化为 0。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e3daedd-9397-4a06-97d7-670919ad85c6",
   "metadata": {},
   "source": [
    "# Q-learning parameters\n",
    "ALPHA = 0.7   # learning rate\n",
    "GAMMA = 0.99  # discount factor\n",
    "\n",
    "def initialize_q_table():\n",
    "  \n",
    "    Q = np.zeros((N_STATES, len(ACTIONS)))\n",
    "    return Q\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d177d58-e02a-4d88-abe1-9744a3319101",
   "metadata": {},
   "source": [
    "## 2.2 Q-learning Update Rule\n",
    "\n",
    "实现基于公式\n",
    "\n",
    "\\\\[\n",
    "Q[s, a] = (1 - \\\\alpha) Q[s, a] + \\\\alpha (r(s') + \\\\gamma \\\\max_{a'} Q[s', a'])\n",
    "\\\\]\n",
    "\n",
    "的更新函数，其中学习率 \\\\(\\\\alpha = 0.7\\\\)，折扣因子 \\\\(\\\\gamma = 0.99\\\\)。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "29a05664-6e86-4fee-b384-02fcd92cb7eb",
   "metadata": {},
   "source": [
    "def q_learning_update(Q, state, action, reward_value, next_state, alpha=ALPHA, gamma=GAMMA):\n",
    "   \n",
    "    # 注意：Python 索引从 0 开始，所以要减 1\n",
    "    s_idx = state - 1\n",
    "    s_next_idx = next_state - 1\n",
    "    a_idx = action  # 这里动作本来就用 0,1,2,3 表示\n",
    "\n",
    "    old_value = Q[s_idx, a_idx]\n",
    "    next_max = np.max(Q[s_next_idx, :])\n",
    "\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward_value + gamma * next_max)\n",
    "    Q[s_idx, a_idx] = new_value\n",
    "\n",
    "    return Q\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0b2dd99-b969-483d-93b6-2836f5ceb1e9",
   "metadata": {},
   "source": [
    "## 2.3 Training with Random Policy\n",
    "\n",
    "使用与 Exercise 1 相同的随机策略进行 20,000 步交互，每一步都根据 Q-learning 公式更新 Q 表。\n",
    "在若干关键步数（如 100, 200, 500, …, 20000）暂停学习，运行测试回合并记录平均奖励。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b609db83-e024-44a4-942e-ed83fb1a5e3e",
   "metadata": {},
   "source": [
    "def run_greedy_test_episode(Q, max_steps=1000):\n",
    "  \n",
    "    current_state = START_STATE\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        s_idx = current_state - 1\n",
    "        q_values = Q[s_idx, :]\n",
    "\n",
    "        # 找到当前状态下 Q 值最大的动作（可能不止一个）\n",
    "        max_q = np.max(q_values)\n",
    "        best_actions = np.where(q_values == max_q)[0]\n",
    "        action = int(np.random.choice(best_actions))\n",
    "\n",
    "        # 与环境交互\n",
    "        next_state = step(current_state, action)\n",
    "        r = reward(next_state)\n",
    "\n",
    "        total_reward += r\n",
    "        steps += 1\n",
    "\n",
    "        if next_state == GOAL_STATE:\n",
    "            breakb\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    avg_reward = total_reward / steps if steps > 0 else 0.0\n",
    "    return avg_reward, steps\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2109ee0-5c9b-4e6b-b6e5-c61f06e0e6d1",
   "metadata": {},
   "source": [
    "# 设定训练的总步数和测试点\n",
    "TOTAL_STEPS = 20000\n",
    "TEST_POINTS = [100, 200, 500, 600, 700, 800, 900, 1000,\n",
    "               2500, 5000, 7500, 10000, 12500, 15000, 17500, 20000]\n",
    "\n",
    "def train_q_learning_random_policy(total_steps=TOTAL_STEPS,\n",
    "                                   test_points=TEST_POINTS,\n",
    "                                   alpha=ALPHA, gamma=GAMMA,\n",
    "                                   max_test_steps=1000):\n",
    "   \n",
    "    Q = initialize_q_table()\n",
    "    current_state = START_STATE\n",
    "\n",
    "    test_avg_rewards = []\n",
    "    next_test_idx = 0  # 当前要检测的 test_points 索引\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step_idx in range(1, total_steps + 1):\n",
    "        # 1. 随机选择一个动作（探索）\n",
    "        action = np.random.choice(ACTIONS)\n",
    "\n",
    "        # 2. 与环境交互，得到下一状态和奖励\n",
    "        next_state = step(current_state, action)\n",
    "        r = reward(next_state)\n",
    "\n",
    "        # 3. Q-learning 更新\n",
    "        Q = q_learning_update(Q, current_state, action, r, next_state,\n",
    "                              alpha=alpha, gamma=gamma)\n",
    "\n",
    "        # 4. 如果到达目标，重置到起始状态；否则继续\n",
    "        if next_state == GOAL_STATE:\n",
    "            current_state = START_STATE\n",
    "        else:\n",
    "            current_state = next_state\n",
    "\n",
    "        # 5. 如果当前步数是一个测试点，进行一次贪心测试 episode\n",
    "        if next_test_idx < len(test_points) and step_idx == test_points[next_test_idx]:\n",
    "            avg_r_test, _ = run_greedy_test_episode(Q, max_steps=max_test_steps)\n",
    "            test_avg_rewards.append(avg_r_test)\n",
    "            next_test_idx += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    return Q, np.array(test_avg_rewards), runtime\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b29c6fb0-2d54-4ade-8277-397b4385d9f6",
   "metadata": {},
   "source": [
    "N_RUNS_Q = 30\n",
    "\n",
    "all_test_rewards = []  # 形状将会是 (30, len(TEST_POINTS))\n",
    "q_runtimes = []\n",
    "\n",
    "for i in range(N_RUNS_Q):\n",
    "    print(f\"Run {i+1}/{N_RUNS_Q} ...\")\n",
    "    Q_final, test_rewards, runtime = train_q_learning_random_policy()\n",
    "\n",
    "    all_test_rewards.append(test_rewards)\n",
    "    q_runtimes.append(runtime)\n",
    "\n",
    "all_test_rewards = np.vstack(all_test_rewards)  # (N_RUNS_Q, len(TEST_POINTS))\n",
    "q_runtimes = np.array(q_runtimes)\n",
    "\n",
    "print(\"Shape of all_test_rewards:\", all_test_rewards.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90c12a0c-2946-4a8e-a67b-da2e8d5491ac",
   "metadata": {},
   "source": [
    "## 2.4 Evaluation Episodes (Greedy Policy)\n",
    "\n",
    "在测试回合中，不再更新 Q 表，而是总是选择具有最大 Q 值的动作（贪心策略），\n",
    "运行最多 1000 步并计算平均奖励，用于评估当前 Q 表的质量。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "58e61972-34e7-4fda-a1d9-a5da0854985d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 计算每个测试点的平均测试奖励和标准差\n",
    "mean_test_rewards = all_test_rewards.mean(axis=0)\n",
    "std_test_rewards = all_test_rewards.std(axis=0)\n",
    "\n",
    "print(\"Test points:\", TEST_POINTS)\n",
    "print(\"Mean test rewards:\", mean_test_rewards)\n",
    "print(\"Std  test rewards:\", std_test_rewards)\n",
    "\n",
    "# 运行时间统计\n",
    "print(\"\\n=== Runtime over Q-learning runs ===\")\n",
    "print(f\"Runtime: mean = {q_runtimes.mean():.4f} s, std = {q_runtimes.std():.4f} s\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2913e644-857d-4e24-8f39-b638376a44fc",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(TEST_POINTS, mean_test_rewards, yerr=std_test_rewards, marker='o')\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Average Reward (test episode)\")\n",
    "plt.title(\"Q-learning Performance (Random Policy Training)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6559dc9-2b70-4c77-b808-cf75a85e553a",
   "metadata": {},
   "source": [
    "## 2.5 Result Visualization\n",
    "\n",
    "1. 绘制“训练步数 vs 测试平均奖励”的曲线；\n",
    "2. 使用热力图展示每个状态的最大 Q 值，以可视化智能体在网格中的价值分布。\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f0631fc-3c01-4768-a863-0e7dccf4609f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 取最近一次 run 得到的 Q_final（如果你想用平均的也可以扩展）\n",
    "Q_learned = Q_final\n",
    "\n",
    "# 对每个状态取 max_a Q[s,a]，并 reshape 成 10×10\n",
    "state_values = np.max(Q_learned, axis=1)          # shape = (100,)\n",
    "value_grid = state_values.reshape((GRID_ROWS, GRID_COLS))\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(value_grid, origin='upper')\n",
    "plt.colorbar(label=\"Max Q-value\")\n",
    "plt.title(\"Learned State Values (Max Q over Actions)\")\n",
    "plt.xlabel(\"Column\")\n",
    "plt.ylabel(\"Row\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2312ae0c-f264-4f7e-bbca-e0a3390c2c55",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
