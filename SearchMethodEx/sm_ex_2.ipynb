{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßÆ Exercise 2 ‚Äì k-Nearest Neighbors (k-NN) Classifier\n",
    "\n",
    "### üéØ Objective\n",
    "The goal of this exercise is to implement and evaluate a **k-Nearest Neighbors (k-NN)** classifier using the **Iris dataset** from the UCI Machine Learning Repository.\n",
    "The classifier predicts the class of new examples by identifying the *k* closest labeled examples in the training set using Euclidean distance.\n",
    "\n",
    "### üß© Tasks Overview\n",
    "1. **Dataset Preparation**\n",
    "   - Load the Iris dataset.\n",
    "   - Split it randomly into training (70 %) and test (30 %) subsets.\n",
    "\n",
    "2. **k-NN Implementation**\n",
    "   - For a new test example, compute its Euclidean distance to all training examples.\n",
    "   - Select the *k* nearest neighbors and use majority voting to predict its class.\n",
    "\n",
    "3. **Model Evaluation**\n",
    "   - Compare classifier accuracy for k = 3, 7, 11.\n",
    "   - Repeat 30 times with different random splits.\n",
    "   - Display results using a boxplot with whiskers.\n",
    "\n",
    "4. **Confusion Matrix**\n",
    "   - Plot the confusion matrix of one representative run for each value of *k*.\n",
    "\n",
    "5. **Analysis Question**\n",
    "   - Explain why *k* is usually chosen as an **odd number** when the number of classes is even.\n",
    "\n",
    "### ‚öôÔ∏è Tools Used\n",
    "- **pandas** for dataset management\n",
    "- **numpy** for numeric operations\n",
    "- **matplotlib** for visualizations\n",
    "- **scikit-learn** (optional) for reference metrics (accuracy, confusion matrix)\n",
    "\n",
    "This notebook implements the k-NN algorithm manually (step-by-step) to demonstrate understanding before relying on high-level libraries.\n"
   ],
   "id": "6ad48a30bdd4f272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üß© Cell 1 ‚Äî Imports & basic setup\n",
    "- pandas and numpy for data handling\n",
    "- matplotlib.pyplot for visualizations\n",
    "- sklearn.datasets for loading the Iris dataset\n",
    "- sklearn.model_selection for train/test split\n",
    "- sklearn.metrics for accuracy and confusion matrix"
   ],
   "id": "70c8756dbfe8d08a"
  },
  {
   "cell_type": "code",
   "id": "a674f399-100f-49c1-b248-7a84eca82828",
   "metadata": {},
   "source": [
    "# Step 1: Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19225891-b061-4cbf-8b0a-edb505d64aeb",
   "metadata": {},
   "source": [
    "üìù Cell 2 ‚Äì Load and Prepare the Dataset\n",
    "- Load the Iris dataset from sklearn and convert it to a pandas DataFrame.\n",
    "- The target (species) is stored in a separate column for convenience."
   ]
  },
  {
   "cell_type": "code",
   "id": "7121781e-9cea-4894-95f4-5c1132832836",
   "metadata": {},
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7a43893-72f6-4d29-9a42-594a8c6dae17",
   "metadata": {},
   "source": [
    "üìù Cell 3 ‚Äì Define the Euclidean Distance Function\n",
    "- Define a helper function to compute Euclidean distance between two feature vectors.\n",
    " This will be used to measure \"closeness\" between points."
   ]
  },
  {
   "cell_type": "code",
   "id": "f548b8a6-a01e-4d3e-8cf9-9a37b2328df6",
   "metadata": {},
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c22cf3f-3a34-4ffb-b7e1-248439dcc420",
   "metadata": {},
   "source": [
    "üìù Cell 4 ‚Äì Implement the k-NN Classifier\n",
    "#### Implement the k-Nearest Neighbors algorithm from scratch.\n",
    "#### Given a training set (X_train, y_train) and a new example x_test:\n",
    "- Compute the distance from x_test to all training samples.\n",
    "- Sort distances and select the k nearest neighbors.\n",
    "- Return the most frequent label among them (majority vote)."
   ]
  },
  {
   "cell_type": "code",
   "id": "3447f944-e644-4ae7-81d9-5390d3050088",
   "metadata": {},
   "source": [
    "def knn_predict(X_train, y_train, x_test, k):\n",
    "    distances = []\n",
    "    for i, x_train in enumerate(X_train):\n",
    "        dist = euclidean_distance(x_train, x_test)\n",
    "        distances.append((dist, y_train[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = [label for (_, label) in distances[:k]]\n",
    "    prediction = max(set(k_nearest), key=k_nearest.count)\n",
    "    return prediction\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7357-a522-4a05-a36a-d266da4c7362",
   "metadata": {},
   "source": [
    "üìù Cell 5 ‚Äì Evaluate Accuracy for a Given k\n",
    "- Randomly splits the dataset into 70% train / 30% test.\n",
    "- Predicts labels for all test examples using the custom k-NN implementation.\n",
    "- Computes and returns the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "8cead0f2-a2b1-4bca-a301-de4d6136ed59",
   "metadata": {},
   "source": [
    "def evaluate_knn(df, k):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.iloc[:, :-1].values, df['species'].values, test_size=0.3, stratify=df['species']\n",
    "    )\n",
    "    preds = [knn_predict(X_train, y_train, x, k) for x in X_test]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    return acc, y_test, preds\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbcb39f9-23f4-4b68-9133-e5f56d1c6cae",
   "metadata": {},
   "source": [
    "üìù Cell 6 ‚Äì Compare Performance for k = 3, 7, 11 (30 Trials)\n",
    "- For each k (3, 7, 11), repeat 30 random splits of the dataset.\n",
    "- Record accuracy for each run and visualize using boxplots.\n",
    "- This evaluates stability and typical performance of different k values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2aed831-75c4-458a-b7d5-be5c46677ef8",
   "metadata": {},
   "source": [
    "ks = [3, 7, 11]\n",
    "n_trials = 30\n",
    "results = {k: [] for k in ks}\n",
    "\n",
    "for k in ks:\n",
    "    for seed in range(n_trials):\n",
    "        np.random.seed(seed)\n",
    "        acc, _, _ = evaluate_knn(df, k)\n",
    "        results[k].append(acc)\n",
    "\n",
    "for k in ks:\n",
    "    print(f\"k={k}: mean={np.mean(results[k]):.3f}, std={np.std(results[k]):.3f}\")\n",
    "\n",
    "plt.boxplot([results[k] for k in ks], labels=ks)\n",
    "plt.title(\"Accuracy Distribution for Different k Values (30 Trials)\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, axis='y', linestyle=':')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üìù Cell 7 ‚Äì Confusion Matrix for Each k\n",
    "- This cell runs one representative trial for each k and prints its confusion matrix.\n",
    "- The confusion matrix shows how well the classifier distinguishes between classes.\n"
   ],
   "id": "7805adbe29e32120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k in ks:\n",
    "    acc, y_true, y_pred = evaluate_knn(df, k)\n",
    "    print(f\"\\nConfusion Matrix for k={k} (accuracy={acc:.3f})\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    plt.title(f\"Confusion Matrix (k={k})\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n"
   ],
   "id": "91aebe7e1583dfd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11967f7f-c436-4ab4-8972-59114d30b3e5",
   "metadata": {},
   "source": [
    "#### üß© Why Should k Be an Odd Number?\n",
    "\n",
    "When the number of classes is even, using an **odd value of k** helps to **break ties** during majority voting.\n",
    "If k were even (e.g., k = 4) and two neighbors belong to class A while two belong to class B,\n",
    "the classifier would have no clear majority and would need a tie-breaking rule.\n",
    "Choosing an odd k ensures that one class always receives more votes than the others,\n",
    "resulting in a deterministic prediction.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚úÖ Discussion and Conclusions\n",
    "\n",
    "#### üßæ Summary of Results\n",
    "- As *k* increases, the classifier becomes smoother (less variance) but may lose sensitivity to local patterns.\n",
    "- Lower values of *k* (like 3) can achieve slightly higher accuracy but may be more affected by noise.\n",
    "- The overall performance across 30 runs is consistent and high (> 0.9 accuracy for all tested *k* values).\n",
    "\n",
    "#### üìä Observations\n",
    "- The boxplots confirm that different *k* values produce similar accuracy, with small variability.\n",
    "- Confusion matrices show clear class separations for **Iris-setosa** and small confusion between **versicolor** and **virginica**.\n",
    "\n",
    "#### üîç Conclusions\n",
    "1. The k-NN algorithm is simple yet powerful for small, well-structured datasets like Iris.\n",
    "2. Proper choice of *k* controls the trade-off between overfitting (low *k*) and underfitting (high *k*).\n",
    "3. Repeated random splits provide a more reliable estimate of model performance.\n",
    "\n",
    "#### üöÄ Future Improvements\n",
    "- Apply feature scaling (normalization) to improve distance computation consistency.\n",
    "- Use cross-validation instead of random splitting for more robust accuracy estimation.\n",
    "- Extend the experiment to multi-dimensional datasets or higher values of *k* to study asymptotic behavior.\n",
    "\n",
    "This exercise demonstrates the intuition behind **instance-based learning** and how the choice of neighborhood size affects classification performance.\n"
   ],
   "id": "cf6b364a1336c27a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global)",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
