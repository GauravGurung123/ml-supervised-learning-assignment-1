{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßÆ Exercise 3 ‚Äì Na√Øve Bayes Classifier (Iris Dataset)\n",
    "\n",
    "### üéØ Objective\n",
    "In this exercise, we implement a **Na√Øve Bayes classifier** using the **Iris dataset**, applying probability theory to predict the class of each example.\n",
    "The Na√Øve Bayes method assumes that all features are independent given the class, allowing for efficient computation of conditional probabilities.\n",
    "\n",
    "### üß© Tasks Overview\n",
    "1. **Data Preparation**\n",
    "   - Load the Iris dataset and discretize all numerical columns into three categories: **low**, **medium**, and **high**.\n",
    "   - Split the dataset randomly into 70 % training and 30 % testing subsets.\n",
    "\n",
    "2. **Na√Øve Bayes Implementation**\n",
    "   - Estimate prior probabilities *P(Class)* for each class.\n",
    "   - Estimate conditional probabilities *P(X·µ¢ | Class)* from the training data.\n",
    "   - For each test instance, compute *P(Class | X)* using Bayes‚Äô theorem and choose the class with the highest probability.\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Repeat 30 random train/test splits.\n",
    "   - Compute accuracy and display a confusion matrix for one representative run.\n",
    "\n",
    "4. **Comparison**\n",
    "   - Compare performance with the k-NN classifier (Exercise 2) and discuss advantages/disadvantages.\n",
    "\n",
    "### ‚öôÔ∏è Tools Used\n",
    "- **pandas** for data manipulation\n",
    "- **numpy** for numeric calculations\n",
    "- **matplotlib** for visualizations\n",
    "- **sklearn.metrics** for evaluation metrics (accuracy, confusion matrix)\n",
    "\n",
    "This implementation is written manually to reinforce understanding of probabilistic reasoning before using automated libraries.\n"
   ],
   "id": "6ad48a30bdd4f272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üß© Cell 1 ‚Äî Imports & basic setup\n",
    "- pandas and numpy for data handling\n",
    "- matplotlib.pyplot for visualizations\n",
    "- sklearn.datasets for loading the Iris dataset\n",
    "- sklearn.model_selection for train/test split\n",
    "- sklearn.metrics for accuracy and confusion matrix"
   ],
   "id": "70c8756dbfe8d08a"
  },
  {
   "cell_type": "code",
   "id": "a674f399-100f-49c1-b248-7a84eca82828",
   "metadata": {},
   "source": [
    "# Import libraries used throughout the experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19225891-b061-4cbf-8b0a-edb505d64aeb",
   "metadata": {},
   "source": [
    "üìù Cell 2 ‚Äì Load and Display the Iris Dataset\n",
    "- Load the Iris dataset and convert it into a pandas DataFrame for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "id": "7121781e-9cea-4894-95f4-5c1132832836",
   "metadata": {},
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7a43893-72f6-4d29-9a42-594a8c6dae17",
   "metadata": {},
   "source": [
    "üìù Cell 3 ‚Äì Discretize Continuous Features\n",
    "- Discretize each numerical feature into three categories: low, medium, high.\n",
    "- We use pandas 'qcut' to split the distribution into three equal-frequency bins."
   ]
  },
  {
   "cell_type": "code",
   "id": "f548b8a6-a01e-4d3e-8cf9-9a37b2328df6",
   "metadata": {},
   "source": [
    "def discretize_features(df):\n",
    "    discretized = df.copy()\n",
    "    for col in iris.feature_names:\n",
    "        discretized[col] = pd.qcut(df[col], q=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "    return discretized\n",
    "\n",
    "df_disc = discretize_features(df)\n",
    "df_disc.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c22cf3f-3a34-4ffb-b7e1-248439dcc420",
   "metadata": {},
   "source": [
    "üìù Cell 4 ‚Äì Split Dataset into Train/Test Sets\n",
    "#### Randomly split the discretized dataset into training (70%) and testing (30%) subsets."
   ]
  },
  {
   "cell_type": "code",
   "id": "3447f944-e644-4ae7-81d9-5390d3050088",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_disc.iloc[:, :-1], df_disc['species'], test_size=0.3, stratify=df_disc['species']\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7357-a522-4a05-a36a-d266da4c7362",
   "metadata": {},
   "source": [
    "üìù Cell 5 ‚Äì Estimate Prior and Conditional Probabilities\n",
    "#### Build dictionaries storing:\n",
    "1. Prior probabilities for each class (P(Class))\n",
    "2. Conditional probabilities for each feature value given a class (P(Xi | Class))\n",
    "\n",
    "#### Laplace smoothing is applied (+1) to avoid zero probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8cead0f2-a2b1-4bca-a301-de4d6136ed59",
   "metadata": {},
   "source": [
    "def train_naive_bayes(X_train, y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    priors = {c: (y_train == c).mean() for c in classes}\n",
    "    cond_probs = {}\n",
    "\n",
    "    for c in classes:\n",
    "        X_c = X_train[y_train == c]\n",
    "        cond_probs[c] = {}\n",
    "        for col in X_train.columns:\n",
    "            value_counts = X_c[col].value_counts()\n",
    "            total = value_counts.sum()\n",
    "            probs = {val: (value_counts.get(val, 0) + 1) / (total + 3) for val in [\"low\", \"medium\", \"high\"]}\n",
    "            cond_probs[c][col] = probs\n",
    "\n",
    "    return priors, cond_probs\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbcb39f9-23f4-4b68-9133-e5f56d1c6cae",
   "metadata": {},
   "source": [
    "üìù Cell 6 ‚Äì Na√Øve Bayes Prediction Function\n",
    "#### For a single sample x, compute the posterior probability for each class:\n",
    "-   P(Class|X) ‚àù P(Class) √ó Œ†_i P(X_i|Class)\n",
    "- Return the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2aed831-75c4-458a-b7d5-be5c46677ef8",
   "metadata": {},
   "source": [
    "def predict_naive_bayes(x, priors, cond_probs):\n",
    "    posteriors = {}\n",
    "    for c in priors.keys():\n",
    "        prob = np.log(priors[c])  # use log to avoid underflow\n",
    "        for feature, value in x.items():\n",
    "            prob += np.log(cond_probs[c][feature].get(value, 1e-6))\n",
    "        posteriors[c] = prob\n",
    "    return max(posteriors, key=posteriors.get)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üìù Cell 7 ‚Äì Confusion Matrix for Each k\n",
    "- This cell runs one representative trial for each k and prints its confusion matrix.\n",
    "- The confusion matrix shows how well the classifier distinguishes between classes.\n"
   ],
   "id": "7805adbe29e32120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for k in ks:\n",
    "    acc, y_true, y_pred = evaluate_knn(df, k)\n",
    "    print(f\"\\nConfusion Matrix for k={k} (accuracy={acc:.3f})\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    plt.title(f\"Confusion Matrix (k={k})\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n"
   ],
   "id": "91aebe7e1583dfd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11967f7f-c436-4ab4-8972-59114d30b3e5",
   "metadata": {},
   "source": [
    "#### üß© Why Should k Be an Odd Number?\n",
    "\n",
    "When the number of classes is even, using an **odd value of k** helps to **break ties** during majority voting.\n",
    "If k were even (e.g., k = 4) and two neighbors belong to class A while two belong to class B,\n",
    "the classifier would have no clear majority and would need a tie-breaking rule.\n",
    "Choosing an odd k ensures that one class always receives more votes than the others,\n",
    "resulting in a deterministic prediction.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ‚úÖ Discussion and Conclusions\n",
    "\n",
    "#### üßæ Summary of Results\n",
    "- As *k* increases, the classifier becomes smoother (less variance) but may lose sensitivity to local patterns.\n",
    "- Lower values of *k* (like 3) can achieve slightly higher accuracy but may be more affected by noise.\n",
    "- The overall performance across 30 runs is consistent and high (> 0.9 accuracy for all tested *k* values).\n",
    "\n",
    "#### üìä Observations\n",
    "- The boxplots confirm that different *k* values produce similar accuracy, with small variability.\n",
    "- Confusion matrices show clear class separations for **Iris-setosa** and small confusion between **versicolor** and **virginica**.\n",
    "\n",
    "#### üîç Conclusions\n",
    "1. The k-NN algorithm is simple yet powerful for small, well-structured datasets like Iris.\n",
    "2. Proper choice of *k* controls the trade-off between overfitting (low *k*) and underfitting (high *k*).\n",
    "3. Repeated random splits provide a more reliable estimate of model performance.\n",
    "\n",
    "#### üöÄ Future Improvements\n",
    "- Apply feature scaling (normalization) to improve distance computation consistency.\n",
    "- Use cross-validation instead of random splitting for more robust accuracy estimation.\n",
    "- Extend the experiment to multi-dimensional datasets or higher values of *k* to study asymptotic behavior.\n",
    "\n",
    "This exercise demonstrates the intuition behind **instance-based learning** and how the choice of neighborhood size affects classification performance.\n"
   ],
   "id": "cf6b364a1336c27a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global)",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
