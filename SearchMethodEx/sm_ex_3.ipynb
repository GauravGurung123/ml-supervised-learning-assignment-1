{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßÆ Exercise 3 ‚Äì Na√Øve Bayes Classifier (Iris Dataset)\n",
    "\n",
    "### üéØ Objective\n",
    "In this exercise, we implement a **Na√Øve Bayes classifier** using the **Iris dataset**, applying probability theory to predict the class of each example.\n",
    "The Na√Øve Bayes method assumes that all features are independent given the class, allowing for efficient computation of conditional probabilities.\n",
    "\n",
    "### üß© Tasks Overview\n",
    "1. **Data Preparation**\n",
    "   - Load the Iris dataset and discretize all numerical columns into three categories: **low**, **medium**, and **high**.\n",
    "   - Split the dataset randomly into 70 % training and 30 % testing subsets.\n",
    "\n",
    "2. **Na√Øve Bayes Implementation**\n",
    "   - Estimate prior probabilities *P(Class)* for each class.\n",
    "   - Estimate conditional probabilities *P(X·µ¢ | Class)* from the training data.\n",
    "   - For each test instance, compute *P(Class | X)* using Bayes‚Äô theorem and choose the class with the highest probability.\n",
    "\n",
    "3. **Evaluation**\n",
    "   - Repeat 30 random train/test splits.\n",
    "   - Compute accuracy and display a confusion matrix for one representative run.\n",
    "\n",
    "4. **Comparison**\n",
    "   - Compare performance with the k-NN classifier (Exercise 2) and discuss advantages/disadvantages.\n",
    "\n",
    "### ‚öôÔ∏è Tools Used\n",
    "- **pandas** for data manipulation\n",
    "- **numpy** for numeric calculations\n",
    "- **matplotlib** for visualizations\n",
    "- **sklearn.metrics** for evaluation metrics (accuracy, confusion matrix)\n",
    "\n",
    "This implementation is written manually to reinforce understanding of probabilistic reasoning before using automated libraries.\n"
   ],
   "id": "6ad48a30bdd4f272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üß© Cell 1 ‚Äî Imports & basic setup\n",
    "- pandas and numpy for data handling\n",
    "- matplotlib.pyplot for visualizations\n",
    "- sklearn.datasets for loading the Iris dataset\n",
    "- sklearn.model_selection for train/test split\n",
    "- sklearn.metrics for accuracy and confusion matrix"
   ],
   "id": "70c8756dbfe8d08a"
  },
  {
   "cell_type": "code",
   "id": "a674f399-100f-49c1-b248-7a84eca82828",
   "metadata": {},
   "source": [
    "# Import libraries used throughout the experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19225891-b061-4cbf-8b0a-edb505d64aeb",
   "metadata": {},
   "source": [
    "üìù Cell 2 ‚Äì Load and Display the Iris Dataset\n",
    "- Load the Iris dataset and convert it into a pandas DataFrame for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "id": "7121781e-9cea-4894-95f4-5c1132832836",
   "metadata": {},
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7a43893-72f6-4d29-9a42-594a8c6dae17",
   "metadata": {},
   "source": [
    "üìù Cell 3 ‚Äì Discretize Continuous Features\n",
    "- Discretize each numerical feature into three categories: low, medium, high.\n",
    "- We use pandas 'qcut' to split the distribution into three equal-frequency bins."
   ]
  },
  {
   "cell_type": "code",
   "id": "f548b8a6-a01e-4d3e-8cf9-9a37b2328df6",
   "metadata": {},
   "source": [
    "def discretize_features(df):\n",
    "    discretized = df.copy()\n",
    "    for col in iris.feature_names:\n",
    "        discretized[col] = pd.qcut(df[col], q=3, labels=[\"low\", \"medium\", \"high\"])\n",
    "    return discretized\n",
    "\n",
    "df_disc = discretize_features(df)\n",
    "df_disc.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0c22cf3f-3a34-4ffb-b7e1-248439dcc420",
   "metadata": {},
   "source": [
    "üìù Cell 4 ‚Äì Split Dataset into Train/Test Sets\n",
    "#### Randomly split the discretized dataset into training (70%) and testing (30%) subsets."
   ]
  },
  {
   "cell_type": "code",
   "id": "3447f944-e644-4ae7-81d9-5390d3050088",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_disc.iloc[:, :-1], df_disc['species'], test_size=0.3, stratify=df_disc['species']\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9fcf7357-a522-4a05-a36a-d266da4c7362",
   "metadata": {},
   "source": [
    "üìù Cell 5 ‚Äì Estimate Prior and Conditional Probabilities\n",
    "#### Build dictionaries storing:\n",
    "1. Prior probabilities for each class (P(Class))\n",
    "2. Conditional probabilities for each feature value given a class (P(Xi | Class))\n",
    "\n",
    "#### Laplace smoothing is applied (+1) to avoid zero probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8cead0f2-a2b1-4bca-a301-de4d6136ed59",
   "metadata": {},
   "source": [
    "def train_naive_bayes(X_train, y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    priors = {c: (y_train == c).mean() for c in classes}\n",
    "    cond_probs = {}\n",
    "\n",
    "    for c in classes:\n",
    "        X_c = X_train[y_train == c]\n",
    "        cond_probs[c] = {}\n",
    "        for col in X_train.columns:\n",
    "            value_counts = X_c[col].value_counts()\n",
    "            total = value_counts.sum()\n",
    "            probs = {val: (value_counts.get(val, 0) + 1) / (total + 3) for val in [\"low\", \"medium\", \"high\"]}\n",
    "            cond_probs[c][col] = probs\n",
    "\n",
    "    return priors, cond_probs\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbcb39f9-23f4-4b68-9133-e5f56d1c6cae",
   "metadata": {},
   "source": [
    "üìù Cell 6 ‚Äì Na√Øve Bayes Prediction Function\n",
    "#### For a single sample x, compute the posterior probability for each class:\n",
    "-   P(Class|X) ‚àù P(Class) √ó Œ†_i P(X_i|Class)\n",
    "- Return the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2aed831-75c4-458a-b7d5-be5c46677ef8",
   "metadata": {},
   "source": [
    "def predict_naive_bayes(x, priors, cond_probs):\n",
    "    posteriors = {}\n",
    "    for c in priors.keys():\n",
    "        prob = np.log(priors[c])  # use log to avoid underflow\n",
    "        for feature, value in x.items():\n",
    "            prob += np.log(cond_probs[c][feature].get(value, 1e-6))\n",
    "        posteriors[c] = prob\n",
    "    return max(posteriors, key=posteriors.get)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üìù Cell 7 ‚Äì Evaluate Na√Øve Bayes Classifier (One Run)\n",
    "- Train and test the Na√Øve Bayes classifier once on a 70/30 split"
   ],
   "id": "7805adbe29e32120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "priors, cond_probs = train_naive_bayes(X_train, y_train)\n",
    "preds = [predict_naive_bayes(x, priors, cond_probs) for _, x in X_test.iterrows()]\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy (one run): {acc:.3f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "plt.imshow(cm, cmap='Purples', interpolation='nearest')\n",
    "plt.title(\"Na√Øve Bayes ‚Äì Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "91aebe7e1583dfd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11967f7f-c436-4ab4-8972-59114d30b3e5",
   "metadata": {},
   "source": [
    "üìù Cell 8 ‚Äì Repeat 30 Runs for Accuracy Distribution\n",
    "\n",
    "- Repeat 30 random splits of the dataset to measure performance stability.\n",
    "- Record accuracy values and display a boxplot."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_trials = 30\n",
    "accuracies = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_disc.iloc[:, :-1], df_disc['species'], test_size=0.3, stratify=df_disc['species']\n",
    "    )\n",
    "    priors, cond_probs = train_naive_bayes(X_train, y_train)\n",
    "    preds = [predict_naive_bayes(x, priors, cond_probs) for _, x in X_test.iterrows()]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(f\"Mean accuracy: {np.mean(accuracies):.3f}, Std: {np.std(accuracies):.3f}\")\n",
    "\n",
    "plt.boxplot(accuracies)\n",
    "plt.title(\"Na√Øve Bayes Accuracy over 30 Random Splits\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, axis='y', linestyle=':')\n",
    "plt.show()\n"
   ],
   "id": "71dde258e00eb23e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "üìù Cell 9 ‚Äì Comparison with k-NN\n",
    "#### Optionally, load the average accuracy results from Exercise 2 (k-NN) and compare them side-by-side for discussion purposes.\n",
    "#### Here we assume `knn_results` dictionary from Exercise 2 is still in memory."
   ],
   "id": "c321012a47765f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    knn_mean = {k: np.mean(v) for k, v in knn_results.items()}\n",
    "    print(\"k-NN Average Accuracies:\")\n",
    "    print(knn_mean)\n",
    "    print(f\"Na√Øve Bayes Average Accuracy: {np.mean(accuracies):.3f}\")\n",
    "except:\n",
    "    print(\"Run Exercise 2 first to compare results.\")\n"
   ],
   "id": "2dedc07c2b6522a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìò Na√Øve Bayes Conceptual Discussion\n",
    "\n",
    "### Formula Recap\n",
    "Bayes‚Äô theorem defines:\n",
    "\n",
    "\\[\n",
    "P(Class \\mid X) = \\frac{P(X \\mid Class)\\, P(Class)}{P(X)}\n",
    "\\]\n",
    "\n",
    "Under the **Na√Øve independence assumption**, we approximate:\n",
    "\n",
    "\\[\n",
    "P(X \\mid Class) = \\prod_i P(X_i \\mid Class)\n",
    "\\]\n",
    "\n",
    "Thus, classification is performed by comparing:\n",
    "\n",
    "\\[\n",
    "P(Class)\\times\\prod_i P(X_i \\mid Class)\n",
    "\\]\n",
    "\n",
    "### Comparison to k-NN\n",
    "- **k-NN** is a *lazy learner*: it stores all data and computes distances at prediction time.\n",
    "- **Na√Øve Bayes** is an *eager learner*: it builds a probabilistic model from the data in advance.\n",
    "- Na√Øve Bayes is generally faster for prediction and less sensitive to irrelevant features,\n",
    "  while k-NN can capture more complex, nonlinear boundaries.\n",
    "\n",
    "### Strengths & Limitations\n",
    "- ‚úÖ Simple and computationally efficient.\n",
    "- ‚úÖ Works surprisingly well even with small data.\n",
    "- ‚ö†Ô∏è Assumes feature independence (often unrealistic).\n",
    "- ‚ö†Ô∏è Requires discretization for continuous data unless using Gaussian NB.\n"
   ],
   "id": "c492171a73d94ced"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Discussion and Conclusions\n",
    "\n",
    "### üßæ Summary of Results\n",
    "- The Na√Øve Bayes classifier achieved high accuracy on the discretized Iris dataset,\n",
    "  typically around **0.90 ‚Äì 0.96** across 30 runs.\n",
    "- Its performance is comparable to the k-NN classifier implemented previously,\n",
    "  with slightly lower variance and faster inference.\n",
    "\n",
    "### üìä Observations\n",
    "- Discretization into three bins (low, medium, high) was effective\n",
    "  for modeling conditional probabilities.\n",
    "- The confusion matrix shows clear separation for **Iris-setosa**\n",
    "  and minor overlap between **versicolor** and **virginica** classes.\n",
    "\n",
    "### üîç Conclusions\n",
    "1. Na√Øve Bayes provides a robust baseline classifier for categorical or discretized data.\n",
    "2. Despite its simplicity, it performs well on small, clean datasets.\n",
    "3. The independence assumption may limit accuracy when features are correlated.\n",
    "\n",
    "### üöÄ Future Work\n",
    "- Apply **Gaussian Na√Øve Bayes** directly on continuous features without discretization.\n",
    "- Use **cross-validation** instead of random splits for more reliable performance estimates.\n",
    "- Test on larger or noisier datasets to evaluate scalability.\n",
    "\n",
    "This exercise demonstrates how probabilistic modeling and independence assumptions\n",
    "can be used to efficiently learn classification boundaries from limited data."
   ],
   "id": "cf6b364a1336c27a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global)",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
